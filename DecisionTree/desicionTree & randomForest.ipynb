{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 決定木(decision tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "質問に対する分岐（基本２択）を階層的に作ることで、判別/回帰を行うモデル\n",
    "\n",
    "- 予測精度はまあまあ\n",
    "- ２択の分岐となるため、人間が理解しやすい\n",
    "- パラメータ調整を上手にしないと過学習しやすい\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "決定木は回帰木と分類木の総称\n",
    "\n",
    "わかりやすい例\n",
    "https://mathwords.net/ketteigi\n",
    "\n",
    "- 分類木\n",
    "\n",
    "\n",
    "\n",
    "- 回帰木"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 決定木のアルゴリズム\n",
    "\n",
    "\n",
    "## C4.5\n",
    "　CARTと異なる点\n",
    " - CARTは2分岐しかできないがC4.5は3分岐以上もできる \n",
    " - 決定木を構築する際にCARTはジニ係数を分割の指標にするが、C4.5は情報量をベースの指標にしている\n",
    " - CARTは決定木の剪定を多項式のクラスバリデーションによって行うため時間がかかるが、C4.5は二項の信頼区間の限界を使うため一方行ででき、時間がかからない\n",
    " \n",
    " ### See5/C5.0\n",
    " バージョンアップ版？\n",
    " 変更点\n",
    " - ブースティングを組み込むことで精度が格段に上がった\n",
    " - 拡張性が向上し、マルチコアCPUでより効果的に。\n",
    "\n",
    "\n",
    "## CART\n",
    "\n",
    "\n",
    "## ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 長所と短所\n",
    "\n",
    "\n",
    "## 長所\n",
    "- 可読性が高い\n",
    "- 説明変数・目的変数ともに名義尺度から間隔尺度まで様々なデータに対して扱える\n",
    "- 外れ値に対して頑健\n",
    "\n",
    "\n",
    "## 短所\n",
    "- 分類性能の高い手法ではない\n",
    "- 過学習しやすい\n",
    "- 線形性のあるデータには適していない\n",
    "- XORなど多変数を考慮した分類はできない\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分類での目的関数と分割条件\n",
    "\n",
    "くわしい\n",
    "https://qiita.com/3000manJPY/items/ef7495960f472ec14377\n",
    "https://spjai.com/regression-tree/\n",
    "## エントロピー\n",
    "エントロピーとは物事の乱雑さを測る指標のこと。\n",
    "散らばり具合が小さいほど、エントロピーは小さくなり、大きいほど大きくなる。\n",
    "\n",
    "\n",
    "## ジニ係数\n",
    "エントロピーと同じ概念で、ノードに含まれるサンプルが全て同じ場合に最も低くなり、\n",
    "ノードの含まれるサンプルが均等に散らばっている場合に最も大きくなる。\n",
    "\n",
    "\n",
    "## 分類誤差\n",
    "ノードに含まれるサンプルの、ある特定のクラスに分類される確率を計算して、それを全体の確率から引いて、誤差を計算する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回帰での目的関数と分割条件\n",
    "\n",
    "\n",
    "## MSE\n",
    "https://spjai.com/regression-tree/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ランダムフォレスト\n",
    "https://spjai.com/regression-tree/\n",
    "\n",
    "復数の決定木を使用して、アンサンブル学習をすること。\n",
    "\n",
    "\n",
    "アンサンブル学習をするにあたって、抽出する説明変数の数\n",
    "説明変数の総数をpとすると、一般的に\n",
    "- 分類の場合は√p\n",
    "- 回帰の場合はp/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ツール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphviz\n",
    "`$ pip install graphviz`\n",
    "1. 分岐の条件\n",
    "2. ジニ係数（偏り具合を表す）\n",
    "3. 該当サンプル数\n",
    "4. 該当サンプルの内訳\n",
    "5. 条件を満たした場合になんと判断するか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pydotplus\n",
    "\n",
    "#### install \n",
    "http://srknr.hatenablog.com/entry/2016/11/01/114028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learnでの実装\n",
    "\n",
    "- 分類の場合は、RandomForestClassifierクラス\n",
    "- 回帰の場合は、RandomForestRegressorクラス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パラメータ\n",
    "\n",
    "\n",
    "\n",
    "#### max_depth\n",
    "- 根（こん）から葉までの深さ\n",
    "- 深ければ深いほど、分岐数も増えるため、説明力が上がる\n",
    "- 深すぎると意味のない分岐もできやすく、過学習のおそれがある\n",
    "\n",
    "\n",
    "#### min_samples_leaf\n",
    "- 葉に所属する最低サンプル数を制御できる\n",
    "- サンプル数が少ない = 信憑性の低い分岐の可能性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "clf1 = DT(max_depth=2, min_samples_leaf=500)\n",
    "clf1.fit(trainX, y)\n",
    "export_graphviz(clf1, out_file=\"tree.dot\", feature_names=trainX.columns, class_names=[\"0\",\"1\"], filled=True, rounded=True)\n",
    "\n",
    "\n",
    "g = pydotplus.graph_from_dot_file(path=\"tree.dot\")\n",
    "Image(g.create_png())\n",
    "\n",
    "pred = clf1.predict_proba(testX)\n",
    "pred = pred[:,1]\n",
    "\n",
    "sample[1] = pred\n",
    "sample.to_csv(\"submit1_bank.csv\", header=None, index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
